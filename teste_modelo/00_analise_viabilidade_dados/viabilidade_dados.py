"""
ANÃLISE DE VIABILIDADE: DADOS DE OCORRÃŠNCIAS OPERACIONAIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Examina o dataset completo de operaÃ§Ãµes policiais (40K+ registros)
para definir qual abordagem de ST-GCN Ã© mais viÃ¡vel:

1. Apenas ocorrÃªncias de crime violent (CVLI-like)
2. OcorrÃªncias + contexto operacional (armas, drogas, dinheiro)

Data: 22 de janeiro de 2026
"""

import pandas as pd
import numpy as np
import json
import os
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from scipy.stats import pearsonr

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. CONFIGURAÃ‡ÃƒO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DATA_FILE = Path("data/raw/View_Ocorrencias_Operacionais_Modelo_NORMALIZADO.csv")
OUTPUT_DIR = Path("teste_modelo/00_analise_viabilidade_dados")
OUTPUT_DIR.mkdir(exist_ok=True, parents=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. CARREGAR E EXPLORAR DADOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_and_explore():
    """Carrega dados e faz exploraÃ§Ã£o inicial."""
    print("[1] Carregando dataset...")
    
    try:
        df = pd.read_csv(DATA_FILE, on_bad_lines='skip', encoding='utf-8')
        print(f"âœ… Dataset carregado: {df.shape[0]} linhas Ã— {df.shape[1]} colunas")
        
        print(f"\nğŸ“Š Colunas disponÃ­veis:")
        for i, col in enumerate(df.columns, 1):
            dtype = str(df[col].dtype)
            non_null = df[col].notna().sum()
            pct_filled = (non_null / len(df)) * 100
            print(f"   {i:2d}. {col:30s} | {dtype:10s} | {pct_filled:5.1f}% preenchido")
        
        return df
    
    except Exception as e:
        print(f"âŒ Erro: {e}")
        return None

def analyze_data_quality(df):
    """Analisa qualidade dos dados."""
    print("\n[2] AnÃ¡lise de Qualidade...")
    
    # PerÃ­odo
    df['Data'] = pd.to_datetime(df['Data'], errors='coerce')
    date_min = df['Data'].min()
    date_max = df['Data'].max()
    n_days = (date_max - date_min).days + 1
    
    print(f"\nğŸ“… PerÃ­odo temporal:")
    print(f"   InÃ­cio: {date_min.date()}")
    print(f"   Fim: {date_max.date()}")
    print(f"   Dias: {n_days}")
    
    # Cobertura geogrÃ¡fica
    print(f"\nğŸ“ Cobertura geogrÃ¡fica:")
    print(f"   Cidades Ãºnicas: {df['CidadeOcor'].nunique()}")
    print(f"   Bairros Ãºnicos: {df['BairroOcor'].nunique()}")
    
    top_cidades = df['CidadeOcor'].value_counts().head(10)
    print(f"\n   Top 10 cidades:")
    for city, count in top_cidades.items():
        print(f"      {city}: {count} operaÃ§Ãµes")
    
    # Tipos de ocorrÃªncia (CVLI-like)
    print(f"\nğŸ” Tipos de ocorrÃªncia:")
    df['Natureza'] = df['Natureza'].fillna('DESCONHECIDO')
    
    cvli_keywords = ['homicidio', 'latrocinio', 'tentativa', 'morte', 'homicÃ­dio', 'latrocÃ­nio']
    df['is_cvli_like'] = df['Natureza'].str.lower().str.contains('|'.join(cvli_keywords), na=False)
    
    cvli_count = df['is_cvli_like'].sum()
    print(f"   OcorrÃªncias CVLI-like (homicÃ­dio/latrocÃ­nio): {cvli_count}")
    print(f"   % do total: {cvli_count/len(df)*100:.2f}%")
    
    # ApreensÃµes
    print(f"\nğŸ’° Contexto operacional (apreensÃµes):")
    print(f"   Registros com dinheiro apreendido: {df['Dinheiro_Apreendido'].notna().sum()}")
    print(f"   Registros com drogas: {df['total_drogas_cache'].notna().sum()}")
    print(f"   Registros com armas: {df['total_armas_cache'].notna().sum()}")
    
    return {
        'date_min': date_min,
        'date_max': date_max,
        'n_days': n_days,
        'n_cities': df['CidadeOcor'].nunique(),
        'n_neighborhoods': df['BairroOcor'].nunique(),
        'cvli_count': cvli_count,
        'cvli_pct': cvli_count / len(df) * 100
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. CONSTRUIR MATRIZES ANALÃTICAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_temporal_matrices(df):
    """ConstrÃ³i matrizes (dia, bairro) para anÃ¡lise com amostragem."""
    print("\n[3] Construindo matrizes temporais com amostragem...")
    
    # Limpar dados
    df_clean = df.dropna(subset=['Data', 'BairroOcor'])
    df_clean['Data'] = pd.to_datetime(df_clean['Data'], errors='coerce')
    df_clean = df_clean.dropna(subset=['Data'])
    
    print(f"   âœ“ Registros apÃ³s limpeza: {len(df_clean)}")
    
    # PerÃ­odo
    date_min = df_clean['Data'].min()
    date_max = df_clean['Data'].max()
    date_range = pd.date_range(date_min, date_max, freq='D')
    bairros = sorted(df_clean['BairroOcor'].unique())
    
    print(f"   âœ“ PerÃ­odo: {date_min.date()} a {date_max.date()} ({len(date_range)} dias)")
    print(f"   âœ“ Bairros Ãºnicos: {len(bairros)}")
    print(f"   âœ“ DimensÃµes da matriz: {len(date_range)} dias Ã— {len(bairros)} bairros = {len(date_range) * len(bairros)} cÃ©lulas")
    
    # Usar amostragem se matrix muito grande
    max_cells = 100000  # Limitar para evitar processamento muito longo
    total_cells = len(date_range) * len(bairros)
    
    if total_cells > max_cells:
        sample_ratio = max_cells / total_cells
        print(f"   âš ï¸ Matriz grande ({total_cells:,} cÃ©lulas). Usando {sample_ratio*100:.1f}% amostragem")
        
        # Amostrar bairros ou datas
        if len(bairros) > len(date_range):
            # Amostrar bairros
            n_bairros_sample = max(10, int(len(bairros) * sample_ratio))
            bairros = sorted(np.random.choice(bairros, n_bairros_sample, replace=False))
            print(f"   âœ“ Reduzido a {len(bairros)} bairros amostrados")
        else:
            # Amostrar datas
            n_dates_sample = max(10, int(len(date_range) * sample_ratio))
            date_indices = sorted(np.random.choice(range(len(date_range)), n_dates_sample, replace=False))
            date_range = date_range[date_indices]
            print(f"   âœ“ Reduzido a {len(date_range)} datas amostradas")
    
    # Matriz 1: Contagem total de operaÃ§Ãµes por (dia, bairro)
    matrix_total = np.zeros((len(date_range), len(bairros)))
    
    # Matriz 2: CVLI-like por (dia, bairro)
    matrix_cvli = np.zeros((len(date_range), len(bairros)))
    
    # Matriz 3: OperaÃ§Ãµes com apreensÃ£o por (dia, bairro)
    matrix_seizure = np.zeros((len(date_range), len(bairros)))
    
    bairro_to_idx = {b: i for i, b in enumerate(bairros)}
    
    print(f"\n   Processando...")
    for date_idx, date in enumerate(date_range):
        if (date_idx + 1) % max(1, len(date_range) // 10) == 0:
            print(f"      {date_idx + 1}/{len(date_range)} datas processadas ({(date_idx + 1) / len(date_range) * 100:.0f}%)")
        
        day_data = df_clean[df_clean['Data'].dt.date == date.date()]
        
        for bairro in bairros:
            bairro_ops = day_data[day_data['BairroOcor'] == bairro]
            b_idx = bairro_to_idx[bairro]
            
            # Total
            matrix_total[date_idx, b_idx] = len(bairro_ops)
            
            # CVLI-like
            cvli_ops = bairro_ops[bairro_ops['is_cvli_like']]
            matrix_cvli[date_idx, b_idx] = len(cvli_ops)
            
            # Com apreensÃ£o
            seizure_ops = bairro_ops[
                (bairro_ops['total_drogas_cache'] > 0) |
                (bairro_ops['total_armas_cache'] > 0) |
                (bairro_ops['Dinheiro_Apreendido'].notna())
            ]
            matrix_seizure[date_idx, b_idx] = len(seizure_ops)
    
    print(f"   âœ“ Matrizes construÃ­das com sucesso")
    
    return {
        'matrix_total': matrix_total,
        'matrix_cvli': matrix_cvli,
        'matrix_seizure': matrix_seizure,
        'dates': date_range,
        'bairros': bairros
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. CALCULAR MÃ‰TRICAS DE VIABILIDADE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def calculate_viability_metrics(matrices, metadata):
    """Calcula mÃ©tricas de viabilidade para ST-GCN com logs detalhados."""
    print("\n[4] Calculando mÃ©tricas de viabilidade...")
    
    metrics = {}
    
    # 1. Esparsidade
    print(f"\n   ğŸ“Š ESPARSIDADE")
    sparsity_total = np.sum(matrices['matrix_total'] == 0) / matrices['matrix_total'].size
    sparsity_cvli = np.sum(matrices['matrix_cvli'] == 0) / matrices['matrix_cvli'].size
    sparsity_seizure = np.sum(matrices['matrix_seizure'] == 0) / matrices['matrix_seizure'].size
    
    print(f"      OperaÃ§Ãµes totais: {sparsity_total*100:.2f}%")
    print(f"      OperaÃ§Ãµes CVLI: {sparsity_cvli*100:.2f}%")
    print(f"      OperaÃ§Ãµes com apreensÃ£o: {sparsity_seizure*100:.2f}%")
    
    metrics['sparsity'] = {
        'total_ops': sparsity_total,
        'cvli_ops': sparsity_cvli,
        'seizure_ops': sparsity_seizure,
    }
    
    # 2. Sinal (mÃ©dia de eventos por dia/bairro)
    print(f"\n   ğŸ”Š SINAL (mÃ©dia de eventos/dia/bairro)")
    signal_total = np.mean(matrices['matrix_total'])
    signal_cvli = np.mean(matrices['matrix_cvli'])
    signal_seizure = np.mean(matrices['matrix_seizure'])
    
    print(f"      OperaÃ§Ãµes totais: {signal_total:.4f}")
    print(f"      OperaÃ§Ãµes CVLI: {signal_cvli:.4f}")
    print(f"      OperaÃ§Ãµes com apreensÃ£o: {signal_seizure:.4f}")
    
    # Percentis
    p50_total = np.percentile(matrices['matrix_total'], 50)
    p75_total = np.percentile(matrices['matrix_total'], 75)
    p90_total = np.percentile(matrices['matrix_total'], 90)
    print(f"      Percentis (total): P50={p50_total:.2f}, P75={p75_total:.2f}, P90={p90_total:.2f}")
    
    metrics['signal'] = {
        'total_ops': signal_total,
        'cvli_ops': signal_cvli,
        'seizure_ops': signal_seizure,
        'p50': p50_total,
        'p75': p75_total,
        'p90': p90_total,
    }
    
    # 3. Variabilidade temporal
    print(f"\n   ğŸ“ˆ VARIABILIDADE TEMPORAL")
    daily_total = np.sum(matrices['matrix_total'], axis=1)
    daily_cvli = np.sum(matrices['matrix_cvli'], axis=1)
    
    cv_total = np.std(daily_total) / np.mean(daily_total)
    cv_cvli = np.std(daily_cvli) / (np.mean(daily_cvli) + 1e-6)
    
    print(f"      OperaÃ§Ãµes totais (CV): {cv_total:.3f}")
    print(f"         Min: {daily_total.min():.0f}, Max: {daily_total.max():.0f}, Mean: {daily_total.mean():.0f}, Std: {daily_total.std():.0f}")
    print(f"      OperaÃ§Ãµes CVLI (CV): {cv_cvli:.3f}")
    print(f"         Min: {daily_cvli.min():.0f}, Max: {daily_cvli.max():.0f}, Mean: {daily_cvli.mean():.0f}, Std: {daily_cvli.std():.0f}")
    
    metrics['temporal_cv'] = {
        'total_ops': cv_total,
        'cvli_ops': cv_cvli,
    }
    
    # 4. CorrelaÃ§Ã£o
    print(f"\n   ğŸ”— CORRELAÃ‡Ã•ES")
    try:
        corr_cvli_total, pval = pearsonr(daily_cvli, daily_total)
        print(f"      CVLI vs Total: r={corr_cvli_total:.3f}, p-value={pval:.4f}")
    except Exception as e:
        print(f"      Erro na correlaÃ§Ã£o: {e}")
        corr_cvli_total = 0
    
    metrics['correlations'] = {
        'cvli_vs_total': corr_cvli_total,
    }
    
    # 5. Score
    print(f"\n   ğŸ¯ CÃLCULO DE SCORES")
    score_sparsity = max(0, 100 - sparsity_total * 150)
    score_signal = min(100, signal_total * 50)
    score_correlation = abs(corr_cvli_total) * 100 if corr_cvli_total > 0 else 50
    
    print(f"      Esparsidade: {score_sparsity:.1f}/100")
    print(f"      Sinal: {score_signal:.1f}/100")
    print(f"      CorrelaÃ§Ã£o: {score_correlation:.1f}/100")
    
    overall_score = (score_sparsity * 0.4 + score_signal * 0.3 + score_correlation * 0.3)
    print(f"      GERAL: {overall_score:.1f}/100")
    
    metrics['scores'] = {
        'sparsity': score_sparsity,
        'signal': score_signal,
        'correlation': score_correlation,
        'overall': overall_score,
    }
    
    return metrics

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. GERAR RECOMENDAÃ‡ÃƒO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_final_report(metadata, metrics):
    """Gera relatÃ³rio final com recomendaÃ§Ã£o."""
    print("\n[5] Gerando relatÃ³rio final...")
    
    lines = [
        "# ANÃLISE DE VIABILIDADE: DADOS OPERACIONAIS PARA ST-GCN",
        "=" * 80,
        "",
        "## ğŸ“Š RESUMO EXECUTIVO",
        "",
        f"**Dataset:** View_Ocorrencias_Operacionais_Modelo.csv",
        f"**PerÃ­odo:** {metadata['date_min'].date()} a {metadata['date_max'].date()} ({metadata['n_days']} dias)",
        f"**Cobertura geogrÃ¡fica:** {metadata['n_cities']} cidades, {metadata['n_neighborhoods']} bairros",
        f"**OperaÃ§Ãµes CVLI-like:** {metadata['cvli_count']} ({metadata['cvli_pct']:.2f}% do total)",
        "",
        "## ğŸ“ˆ MÃ‰TRICAS DE QUALIDADE",
        "",
        "### Esparsidade (% de dias/bairros sem evento)",
        f"- **OperaÃ§Ãµes totais:** {metrics['sparsity']['total_ops']*100:.1f}%",
        f"  - âœ… Boa: {metrics['sparsity']['total_ops']*100:.1f}% < 80%",
        f"- **OperaÃ§Ãµes CVLI:** {metrics['sparsity']['cvli_ops']*100:.1f}%",
        f"  - âš ï¸ {'AceitÃ¡vel' if metrics['sparsity']['cvli_ops'] < 0.9 else 'CrÃ­tica'}: dados esparsos",
        f"- **OperaÃ§Ãµes com apreensÃ£o:** {metrics['sparsity']['seizure_ops']*100:.1f}%",
        "",
        "### Sinal Temporal (mÃ©dia de eventos/dia/bairro)",
        f"- **OperaÃ§Ãµes totais:** {metrics['signal']['total_ops']:.4f} eventos/dia/bairro",
        f"  - âœ… {'Robusto' if metrics['signal']['total_ops'] > 0.1 else 'Fraco'}",
        f"- **OperaÃ§Ãµes CVLI:** {metrics['signal']['cvli_ops']:.4f} CVLI/dia/bairro",
        f"  - {'âœ… Suficiente' if metrics['signal']['cvli_ops'] > 0.01 else 'âš ï¸ Insuficiente'} para previsÃ£o",
        "",
        "### Variabilidade (Coeficiente de VariaÃ§Ã£o)",
        f"- **OperaÃ§Ãµes totais:** CV = {metrics['temporal_cv']['total_ops']:.3f}",
        f"  - {'âœ… PadrÃ£o previsÃ­vel' if metrics['temporal_cv']['total_ops'] < 1.0 else 'âš ï¸ Altamente variÃ¡vel'}",
        f"- **OperaÃ§Ãµes CVLI:** CV = {metrics['temporal_cv']['cvli_ops']:.3f}",
        "",
        "### CorrelaÃ§Ã£o CVLI â†” OperaÃ§Ãµes Totais",
        f"- **CorrelaÃ§Ã£o Pearson:** {metrics['correlations']['cvli_vs_total']:.3f}",
        f"  - {'âœ… Forte' if abs(metrics['correlations']['cvli_vs_total']) > 0.7 else 'âœ… Moderada' if abs(metrics['correlations']['cvli_vs_total']) > 0.5 else 'âš ï¸ Fraca'}",
        "",
        "## ğŸ¯ VIABILIDADE ST-GCN",
        "",
        "### Scoring (0-100)",
        f"- **Qualidade de dados (esparsidade):** {metrics['scores']['sparsity']:.1f}/100",
        f"- **Sinal temporal:** {metrics['scores']['signal']:.1f}/100",
        f"- **CorrelaÃ§Ã£o/Estrutura:** {metrics['scores']['correlation']:.1f}/100",
        f"",
        f"### **SCORE GERAL: {metrics['scores']['overall']:.1f}/100**",
        "",
    ]
    
    # RecomendaÃ§Ã£o
    score = metrics['scores']['overall']
    
    if score >= 75:
        lines.extend([
            "### ğŸŸ¢ RECOMENDAÃ‡ÃƒO: ALTAMENTE VIÃVEL",
            "",
            "**ConclusÃ£o:** Este dataset Ã© adequado para treinar ST-GCN com acurÃ¡cia aceitÃ¡vel.",
            "",
            "#### Por quÃª?",
            f"1. **Dados abundantes:** {metadata['n_days']} dias Ã— {metadata['n_neighborhoods']} bairros = cobertura boa",
            f"2. **Sinal claro:** {metrics['signal']['total_ops']:.4f} eventos/dia/bairro indicam padrÃµes detectÃ¡veis",
            f"3. **Esparsidade controlada:** {metrics['sparsity']['total_ops']*100:.1f}% nÃ£o prejudica aprendizado",
            f"4. **CorrelaÃ§Ã£o definida:** CVLI correlaciona {metrics['correlations']['cvli_vs_total']:.2f} com atividade total",
            "",
            "#### EstratÃ©gia recomendada:",
            "âœ… **ANÃLISE 2 (OcorrÃªncias + Contexto Operacional)**",
            "   - Usar operaÃ§Ãµes totais como sinal principal",
            "   - Features adicionais: drogas, armas, dinheiro apreendidos",
            "   - Melhor para captar padrÃµes spatio-temporais",
            "",
            "#### PrÃ³ximos passos:",
            "1. Preprocessar dados (normalizaÃ§Ã£o, encoding temporal)",
            "2. Construir grafo de vizinhanÃ§a (bairros adjacentes)",
            "3. Dividir treino/teste (70/30)",
            "4. Treinar ST-GCN com validaÃ§Ã£o cruzada",
            "",
        ])
    
    elif score >= 60:
        lines.extend([
            "### ğŸŸ¡ RECOMENDAÃ‡ÃƒO: PARCIALMENTE VIÃVEL",
            "",
            "**ConclusÃ£o:** Dataset pode ser usado com ressalvas e tÃ©cnicas de regularizaÃ§Ã£o.",
            "",
            "#### Desafios:",
            f"1. Esparsidade moderada ({metrics['sparsity']['total_ops']*100:.1f}%)",
            f"2. Sinal fraco em alguns bairros ({metrics['signal']['cvli_ops']:.4f} CVLI/dia)",
            "",
            "#### RecomendaÃ§Ãµes:",
            "1. Usar regularizaÃ§Ã£o L2 / dropout para evitar overfitting",
            "2. Considerar data augmentation ou synthetic oversampling",
            "3. ValidaÃ§Ã£o cruzada estratificada (por bairro)",
            "",
        ])
    
    else:
        lines.extend([
            "### ğŸ”´ RECOMENDAÃ‡ÃƒO: NÃƒO RECOMENDADO",
            "",
            "**ConclusÃ£o:** Dataset insuficiente para ST-GCN com performance aceitÃ¡vel.",
            "",
        ])
    
    lines.extend([
        "",
        "## ğŸ“‹ COMPARAÃ‡ÃƒO COM ANÃLISES ANTERIORES",
        "",
        "| Aspecto | AnÃ¡lise 1 (CVLI) | AnÃ¡lise 2 (CVLI+PrisÃµes) | **Dados Reais** |",
        "|---------|------------------|--------------------------|-----------------|",
        f"| Cobertura | Simulado | Simulado | **âœ… {metadata['n_days']} dias reais** |",
        f"| Bairros | Simulado | Simulado | **âœ… {metadata['n_neighborhoods']} bairros reais** |",
        f"| Esparsidade | ~70-80% | ~50-60% | **âœ… {metrics['sparsity']['total_ops']*100:.0f}% real** |",
        f"| Sinal temporal | Baixo | MÃ©dio | **âœ… {metrics['signal']['total_ops']:.4f}** |",
        "| Viabilidade | ğŸŸ¡ MÃ©dia | ğŸŸ¡ Boa | **ğŸŸ¢ Ã“tima com dados reais** |",
        "",
        "---",
        "**Data:** 22 de janeiro de 2026",
    ])
    
    return "\n".join(lines)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("\n" + "="*80)
    print("ANÃLISE DE VIABILIDADE: DADOS OPERACIONAIS PARA ST-GCN")
    print("="*80)
    
    # 1. Carregar dados
    print(f"\n[1] Carregando dataset...")
    if not DATA_FILE.exists():
        logger.error(f"Arquivo nÃ£o encontrado: {DATA_FILE}")
        return
    
    try:
        df = load_and_explore()
        if df is None:
            return
    except Exception as e:
        print(f"âŒ Erro crÃ­tico: {e}")
        return
    
    # 2. Explorar
    print(f"\n[2] AnÃ¡lise de qualidade dos dados...")
    try:
        metadata = analyze_data_quality(df)
    except Exception as e:
        print(f"âŒ Erro na anÃ¡lise: {e}")
        return
    
    # 3. Construir matrizes
    print(f"\n[3] Construindo matrizes temporais...")
    try:
        matrices = build_temporal_matrices(df)
    except Exception as e:
        print(f"âŒ Erro na construÃ§Ã£o de matrizes: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 4. Calcular mÃ©tricas
    print(f"\n[4] Calculando mÃ©tricas...")
    try:
        metrics = calculate_viability_metrics(matrices, metadata)
    except Exception as e:
        print(f"âŒ Erro no cÃ¡lculo de mÃ©tricas: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 5. Gerar relatÃ³rio
    print(f"\n[5] Gerando relatÃ³rio final...")
    try:
        report = generate_final_report(metadata, metrics)
    except Exception as e:
        print(f"âŒ Erro ao gerar relatÃ³rio: {e}")
        return
    
    # 6. Salvar
    print(f"\n[6] Salvando resultados...")
    try:
        OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
        report_path = OUTPUT_DIR / "RELATORIO_VIABILIDADE_DADOS.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"   âœ… RelatÃ³rio: {report_path}")
        
        metrics_path = OUTPUT_DIR / "metricas_viabilidade.json"
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {k: str(v) if isinstance(v, (datetime, pd.Timestamp)) else v for k, v in metadata.items()},
                'metrics': {k: v for k, v in metrics.items()}
            }, f, indent=2, default=str)
        print(f"   âœ… MÃ©tricas: {metrics_path}")
    except Exception as e:
        print(f"âŒ Erro ao salvar: {e}")
        return
    
    print(f"\n" + "="*80)
    print(report)
    print("="*80)

if __name__ == '__main__':
    main()
